# -*- coding: utf-8 -*-
"""NLP_exp3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YdiyOYojxZ19YgW2An49Cz7G__pTEuDx
"""

#1.RULE BASED AND HMM

# Importing libraries
import nltk
import numpy as np
import pandas as pd
import random
from sklearn.model_selection import train_test_split
import pprint, time
 
#download the treebank corpus from nltk
nltk.download('treebank')
 
#download the universal tagset from nltk
nltk.download('universal_tagset')
 
# reading the Treebank tagged sentences
nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))
 
#print the first two sentences along with tags
print(nltk_data[:2])

#print each word with its respective tag for first two sentences
for sent in nltk_data[:2]:
  for tuple in sent:
    print(tuple)

# split data into training and validation set in the ratio 80:20
train_set,test_set =train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 101)

# create list of train and test tagged words
train_tagged_words = [ tup for sent in train_set for tup in sent ]
test_tagged_words = [ tup for sent in test_set for tup in sent ]
print(len(train_tagged_words))
print(len(test_tagged_words))

# check some of the tagged words.
train_tagged_words[:5]

#use set datatype to check how many unique tags are present in training data
tags = {tag for word,tag in train_tagged_words}
print(len(tags))
print(tags)
 
# check total words in vocabulary
vocab = {word for word,tag in train_tagged_words}

# compute Emission Probability
def word_given_tag(word, tag, train_bag = train_tagged_words):
    tag_list = [pair for pair in train_bag if pair[1]==tag]
    count_tag = len(tag_list)#total number of times the passed tag occurred in train_bag
    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]
#now calculate the total number of times the passed word occurred as the passed tag.
    count_w_given_tag = len(w_given_tag_list)
 
     
    return (count_w_given_tag, count_tag)

# compute  Transition Probability
def t2_given_t1(t2, t1, train_bag = train_tagged_words):
    tags = [pair[1] for pair in train_bag]
    count_t1 = len([t for t in tags if t==t1])
    count_t2_t1 = 0
    for index in range(len(tags)-1):
        if tags[index]==t1 and tags[index+1] == t2:
            count_t2_t1 += 1
    return (count_t2_t1, count_t1)



# creating t x t transition matrix of tags, t= no of tags
# Matrix(i, j) represents P(jth tag after the ith tag)
 
tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')
for i, t1 in enumerate(list(tags)):
    for j, t2 in enumerate(list(tags)): 
        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]
 
print(tags_matrix)

# convert the matrix to a df for better readability
#the table is same as the transition table shown in section 3 of article
tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))
display(tags_df)

def Viterbi(words, train_bag = train_tagged_words):
    state = []
    T = list(set([pair[1] for pair in train_bag]))
     
    for key, word in enumerate(words):
        #initialise list of probability column for a given observation
        p = [] 
        for tag in T:
            if key == 0:
                transition_p = tags_df.loc['.', tag]
            else:
                transition_p = tags_df.loc[state[-1], tag]
                 
            # compute emission and state probabilities
            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]
            state_probability = emission_p * transition_p    
            p.append(state_probability)
             
        pmax = max(p)
        # getting state for which probability is maximum
        state_max = T[p.index(pmax)] 
        state.append(state_max)
    return list(zip(words, state))

# Let's test our Viterbi algorithm on a few sample sentences of test dataset
random.seed(1234)      #define a random seed to get same sentences when run multiple times
 
# choose random 10 numbers
rndom = [random.randint(1,len(test_set)) for x in range(10)]
 
# list of 10 sents on which we test the model
test_run = [test_set[i] for i in rndom]
 
# list of tagged words
test_run_base = [tup for sent in test_run for tup in sent]
 
# list of untagged words
test_tagged_words = [tup[0] for sent in test_run for tup in sent]

#Here We will only test 10 sentences to check the accuracy
#as testing the whole training set takes huge amount of time
start = time.time()
tagged_seq = Viterbi(test_tagged_words)
end = time.time()
difference = end-start
 
print("Time taken in seconds: ", difference)
 
# accuracy
check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] 
 
accuracy = len(check)/len(tagged_seq)
print('Viterbi Algorithm Accuracy: ',accuracy*100)

#Code to test all the test sentences
#(takes alot of time to run s0 we wont run it here)
# tagging the test sentences()
test_tagged_words = [tup for sent in test_set for tup in sent]
test_untagged_words = [tup[0] for sent in test_set for tup in sent]
test_untagged_words
 
start = time.time()
tagged_seq = Viterbi(test_untagged_words)
end = time.time()
difference = end-start
 
print("Time taken in seconds: ", difference)
 
# accuracy
check = [i for i, j in zip(test_tagged_words, test_untagged_words) if i == j] 
 
accuracy = len(check)/len(tagged_seq)
print('Viterbi Algorithm Accuracy: ',accuracy*100)

#To improve the performance,we specify a rule base tagger for unknown words 
# specify patterns for tagging
patterns = [
    (r'.*ing$', 'VERB'),              # gerund
    (r'.*ed$', 'VERB'),               # past tense 
    (r'.*es$', 'VERB'),               # verb    
    (r'.*\'s$', 'NOUN'),              # possessive nouns
    (r'.*s$', 'NOUN'),                # plural nouns
    (r'\*T?\*?-[0-9]+$', 'X'),        # X
    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers
    (r'.*', 'NOUN')                   # nouns
]
 
# rule based tagger
rule_based_tagger = nltk.RegexpTagger(patterns)

#modified Viterbi to include rule based tagger in it
def Viterbi_rule_based(words, train_bag = train_tagged_words):
    state = []
    T = list(set([pair[1] for pair in train_bag]))
     
    for key, word in enumerate(words):
        #initialise list of probability column for a given observation
        p = [] 
        for tag in T:
            if key == 0:
                transition_p = tags_df.loc['.', tag]
            else:
                transition_p = tags_df.loc[state[-1], tag]
                 
            # compute emission and state probabilities
            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]
            state_probability = emission_p * transition_p    
            p.append(state_probability)
             
        pmax = max(p)
        state_max = rule_based_tagger.tag([word])[0][1]       
        
         
        if(pmax==0):
            state_max = rule_based_tagger.tag([word])[0][1] # assign based on rule based tagger
        else:
            if state_max != 'X':
                # getting state for which probability is maximum
                state_max = T[p.index(pmax)]                
             
         
        state.append(state_max)
    return list(zip(words, state))

#Check how a sentence is tagged by the two POS taggers
#and compare them
test_sent="Will can see Marry"
pred_tags_rule=Viterbi_rule_based(test_sent.split())
pred_tags_withoutRules= Viterbi(test_sent.split())
print(pred_tags_rule)
print(pred_tags_withoutRules)
#Will and Marry are tagged as NUM as they are unknown words for Viterbi Algorithm

#3.BRill
from collections import defaultdict


# A class to hold the information about the tuples
class TaggerTuple:
    def __init__(self, from_tag, to_tag, pre_tag, score):
        self.from_tag = from_tag
        self.to_tag = to_tag
        self.pre_tag = pre_tag
        self.score = score


# A function to store the given corpus into string
def read_file(filename):
    corpus_line = ""

    with open(filename) as file:
        for line in file:
            corpus_line = corpus_line + line
    file.close()

    return corpus_line


# Tokenize input file and create a unigram model
def tokenize(corpus_line):
    unigram = defaultdict(dict)
    unigram_tokens = {}

    unigram_file = open("output\\unigram\\unigram.txt", "w")
    unigram_tokens_file = open("output\\unigram\\unigram_tokens.txt", "w")

    for word in corpus_line.split():
        words = word.split("_")

        if words[0] in unigram:
            if words[1] in unigram[words[0]]:
                unigram[words[0]][words[1]] = unigram[words[0]][words[1]] + 1
            else:
                unigram[words[0]][words[1]] = 1
        else:
            unigram[words[0]][words[1]] = 1

        if words[0] in unigram_tokens:
            unigram_tokens[words[0]] = unigram_tokens[words[0]] + 1
        else:
            unigram_tokens[words[0]] = 1

    for key, value in unigram.items():
        unigram_file.write(key + " " + str(value) + "\n")
    unigram_file.close()

    for key, value in unigram_tokens.items():
        unigram_tokens_file.write(key + " " + str(value) + "\n")
    unigram_tokens_file.close()

    return unigram


# Initialize the dummy corpus with mostly like tags
def initialize_with_most_likely_tag():
    most_likely_unigram = {}

    with open("output\\unigram\most_probable_unigram.txt", "w") as most_likely_unigram_file:
        for key, value in unigram.items():
            sorted_list = sorted(value, key=value.get, reverse=True)
            most_likely_unigram[key] = sorted_list[0]
            most_likely_unigram_file.write(key + " " + str(sorted_list[0]) + "\n")
    most_likely_unigram_file.close()

    return most_likely_unigram


# Train the model to generate 10 transformational templates
def tbl(most_likely_unigram, corpus_tuple, current_tag):
    n = 1
    transforms_queue = []

    while n <= 10:
        best_transform = get_best_transform(most_likely_unigram, corpus_tuple, correct_tag, current_tag, n)

        if best_transform.from_tag == '' and best_transform.to_tag == '':
            break

        apply_transform(best_transform, corpus_tuple, current_tag, n)
        transforms_queue.append(best_transform)
        n = n + 1

    return transforms_queue


# A function to get the best transform
def get_best_transform(most_likely_unigram, corpus_tuple, correct_tag, current_tag, n):
    instance = get_best_instance(most_likely_unigram, corpus_tuple, correct_tag, current_tag, n)
    return instance


# A function to get the best instance
def get_best_instance(most_likely_unigram, corpus_tuple, correct_tag, current_tag, iteration):
    best_score = 0
    all_tags = ["NN", "VB"]

    transform = TaggerTuple("", "", "", "")

    print("Iteration :: " + str(iteration))

    for from_tag in all_tags:
        for to_tag in all_tags:
            max_difference = 0
            num_good_transform = {}
            num_bad_transform = {}

            if from_tag == to_tag:
                continue

            for pos in range(1, len(corpus_tuple)):

                if to_tag == correct_tag[pos] and from_tag == current_tag[pos]:
                    rule = (current_tag[pos - 1], from_tag, to_tag)

                    if rule in num_good_transform:
                        num_good_transform[rule] += 1
                    else:
                        num_good_transform[rule] = 1
                elif from_tag == correct_tag[pos] and from_tag == current_tag[pos]:
                    rule = (current_tag[pos - 1], from_tag, to_tag)

                    if rule in num_bad_transform:
                        num_bad_transform[rule] += 1
                    else:
                        num_bad_transform[rule] = 1

            for key, value in num_good_transform.items():
                if key in num_bad_transform:
                    difference = num_good_transform[key] - num_bad_transform[key]
                else:
                    difference = num_good_transform[key]

                if difference > max_difference:
                    arg_max = key[0]
                    max_difference = difference

            if max_difference > best_score:
                best_rule = "Change tag FROM :: '" + from_tag + "' TO :: '" + to_tag + "' PREV tag :: '" + arg_max + "'"
                best_score = max_difference

                print("Best Rule :: " + best_rule)
                transform = TaggerTuple(from_tag, to_tag, arg_max, best_score)

    return transform


# Apply transform after calculating best score of transformation template
def apply_transform(best_transform, corpus_tuple, current_tag, n):
    current_tag_File = open("output\logs\iteration_" + str(n) + ".txt", "w")

    for pos in range(1, len(corpus_tuple)):
        if (current_tag[pos] == best_transform.from_tag) and (current_tag[pos - 1] == best_transform.pre_tag):
            current_tag[pos] = best_transform.to_tag

    for pos in range(0, len(current_tag)):
        current_tag_File.write(current_tag[pos] + "\n")


# Divide the corpus into 3 forms
# corpus_tuple : all the corpus words
# correct_tag :  all the corpus tags
# current_tag_File : most likely tag applied to all the words in corpus
def create_corpus_tuple(corpus_line, most_likely_unigram):
    corpus_tuple = []
    correct_tag = []
    current_tag = []

    corpus_tuple_file = open("output\\tuple\corpus_tuple.txt", "w")
    correct_tag_file = open("output\\tags\correct_tag.txt", "w")
    current_tag_file = open("output\\tags\current_tag.txt", "w")

    for word in corpus_line.split():
        words = word.split("_")

        corpus_tuple.append(words[0])
        correct_tag.append(words[1])
        current_tag.append(most_likely_unigram[words[0]])

        corpus_tuple_file.write(words[0] + "\n")
        correct_tag_file.write(words[1] + "\n")
        current_tag_file.write(most_likely_unigram[words[0]] + "\n")

    return corpus_tuple, correct_tag, current_tag


# sort all the transformation generated in oprder of their score
def sort_transformation_in_order_of_score(transformation_transforms_queue):
    sorted_Templates = sorted(transformation_transforms_queue, key=lambda x: x.score, reverse=True)
    index = 1

    with open("output\\top10.txt", "w") as top10_file:
        for transformation in sorted_Templates:
            result = str(index) + ":: From '" + transformation.from_tag + "' To '" + transformation.to_tag\
                     + "' when Prev '" + transformation.pre_tag + "'"
            print(result)
            top10_file.write(result + "\n")
            index = index + 1
    top10_file.close()

    return sorted_Templates


filename = "POSTaggedTrainingSet.txt"

corpus_line = read_file(filename)
unigram = tokenize(corpus_line)

most_likely_unigram = initialize_with_most_likely_tag()
corpus_tuple, correct_tag, current_tag = create_corpus_tuple(corpus_line, most_likely_unigram)
transformation_transforms_queue = tbl(most_likely_unigram, corpus_tuple, current_tag)

print("\n================== Top 10 Rules ==================")
sorted_Templates = sort_transformation_in_order_of_score(transformation_transforms_queue)

import nltk
nltk.download('indian')
 
nltk.download('punkt')

#4.POS Tagging for hindi
import nltk
from nltk.corpus import indian
from nltk.tag import tnt
import string


#nltk.download('punkt')
#nltk.download()

tagged_set = 'hindi.pos'
word_set = indian.sents(tagged_set)
count = 0
for sen in word_set:
    count = count + 1
    sen = "".join([" "+i if not i.startswith("'") and i not in string.punctuation else i for i in sen]).strip()
    #print (sen)
#print (count)

train_perc = .9

train_rows = int(train_perc*count)
test_rows = train_rows + 1

print (train_rows, test_rows)

data = indian.tagged_sents(tagged_set)
train_data = data[:train_rows]
test_data = data[test_rows:]


pos_tagger = tnt.TnT()
pos_tagger.train(train_data)
pos_tagger.evaluate(test_data)

word_to_be_tagged = u"इमरान नजीर ने ३२, सलीम ईलाही ने ३० और यूसुफ योहाना ने ४७ रनों की पारी खेली ।"

tokenized = nltk.word_tokenize(word_to_be_tagged)


print(pos_tagger.tag(tokenized))



#5.POS Tagging for hindi and telungu
import nltk
from nltk.corpus import indian
from nltk.tag import tnt
import string


#nltk.download('punkt')
#nltk.download()

tagged_set = 'hindi.pos'
word_set = indian.sents()
count = 0
for sen in word_set:
    count = count + 1
    sen = "".join([" "+i if not i.startswith("'") and i not in string.punctuation else i for i in sen]).strip()
    #print (sen)
#print (count)

train_perc = .9

train_rows = int(train_perc*count)
test_rows = train_rows + 1

print (train_rows, test_rows)

data = indian.tagged_sents()
train_data = data[:train_rows]
test_data = data[test_rows:]


pos_tagger = tnt.TnT()
pos_tagger.train(train_data)
pos_tagger.evaluate(test_data)

word_to_be_tagged = u"ప్రణాళిక ప్రకారం పని సాధించడానికి సిబ్బంది లో ఎవరు ఏపని చేయాలి, ఆడిట్ లక్ష్యాలేమిటి నిర్ణయించి ఆడిట్ కార్యక్రమం తయారుచేయాలి. सईद अनवर ने ५७, इमरान नजीर ने ३२, सलीम ईलाही ने ३० और यूसुफ योहाना ने ४७ रनों की पारी खेली ।"

tokenized = nltk.word_tokenize(word_to_be_tagged)


print(pos_tagger.tag(tokenized))