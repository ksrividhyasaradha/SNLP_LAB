# -*- coding: utf-8 -*-
"""NLP_EX_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gDaS1eIoGTH45d9DxAXCn_vux1Py_O6A
"""

#1
import nltk
from nltk.corpus import wordnet
synonyms = []
antonyms = []
  
for syn in wordnet.synsets("arrive"):
    for l in syn.lemmas():
        synonyms.append(l.name())
        
  
print(set(synonyms))
synonyms.clear()

for syn in wordnet.synsets("mouse"):
    for l in syn.lemmas():
        synonyms.append(l.name())
        
  
print(set(synonyms))
synonyms.clear()


for syn in wordnet.synsets("play"):
    for l in syn.lemmas():
        synonyms.append(l.name())
        
  
print(set(synonyms))
synonyms.clear()


for syn in wordnet.synsets("apple"):
    for l in syn.lemmas():
        synonyms.append(l.name())
        
  
print(set(synonyms))
synonyms.clear()


for syn in wordnet.synsets("tree"):
    for l in syn.lemmas():
        synonyms.append(l.name())
        
  
print(set(synonyms))
synonyms.clear()

import nltk
nltk.download('wordnet')

import nltk
from nltk.corpus import wordnet

antonyms = []

for syn in wordnet.synsets("accept"):
    for l in syn.lemmas():
        synonyms.append(l.name())
        if l.antonyms():
            antonyms.append(l.antonyms()[0].name())
			


print(set(antonyms))
antonyms.clear()




for syn in wordnet.synsets("sunny"):
    for l in syn.lemmas():
        synonyms.append(l.name())
        if l.antonyms():
            antonyms.append(l.antonyms()[0].name())
			


print(set(antonyms))
antonyms.clear()




for syn in wordnet.synsets("liquid accurate"):
    for l in syn.lemmas():
        synonyms.append(l.name())
        if l.antonyms():
            antonyms.append(l.antonyms()[0].name())
			


print(set(antonyms))
antonyms.clear()


for syn in wordnet.synsets("happy"):
    for l in syn.lemmas():
        synonyms.append(l.name())
        if l.antonyms():
            antonyms.append(l.antonyms()[0].name())
			


print(set(antonyms))
antonyms.clear()

#2


from nltk.corpus import wordnet 
#from nltk.tokenize.punkt import PunktWordTokenizer
import sys
from nltk.tokenize import WordPunctTokenizer

functionwords = ['about', 'across', 'against', 'along', 'around', 'at',
                 'behind', 'beside', 'besides', 'by', 'despite', 'down',
                 'during', 'for', 'from', 'in', 'inside', 'into', 'near', 'of',
                 'off', 'on', 'onto', 'over', 'through', 'to', 'toward',
                 'with', 'within', 'without', 'anything', 'everything',
                 'anyone', 'everyone', 'ones', 'such', 'it', 'itself',
                 'something', 'nothing', 'someone', 'the', 'some', 'this',
                 'that', 'every', 'all', 'both', 'one', 'first', 'other',
                 'next', 'many', 'much', 'more', 'most', 'several', 'no', 'a',
                 'an', 'any', 'each', 'no', 'half', 'twice', 'two', 'second',
                 'another', 'last', 'few', 'little', 'less', 'least', 'own',
                 'and', 'but', 'after', 'when', 'as', 'because', 'if', 'what',
                 'where', 'which', 'how', 'than', 'or', 'so', 'before', 'since',
                 'while', 'although', 'though', 'who', 'whose', 'can', 'may',
                 'will', 'shall', 'could', 'be', 'do', 'have', 'might', 'would',
                 'should', 'must', 'here', 'there', 'now', 'then', 'always',
                 'never', 'sometimes', 'usually', 'often', 'therefore',
                 'however', 'besides', 'moreover', 'though', 'otherwise',
                 'else', 'instead', 'anyway', 'incidentally', 'meanwhile']

def overlapcontext( synset, sentence ):
    gloss = set(WordPunctTokenizer().tokenize(synset.definition()))
    for i in synset.examples():
         gloss.union(i)
    gloss = gloss.difference( functionwords )
    if isinstance(sentence, str):
        sentence = set(sentence.split(" "))
    elif isinstance(sentence, list):
        sentence = set(sentence)
    elif isinstance(sentence, set):
        pass
    else:
        return
    sentence = sentence.difference( functionwords )
    return len( gloss.intersection(sentence) )

def lesk( word, sentence ):
    bestsense = None
    maxoverlap = 0
    word=wordnet.morphy(word) if wordnet.morphy(word) is not None else word
    for sense in wordnet.synsets(word):
        overlap = overlapcontext(sense,sentence)
        for h in sense.hyponyms():
            overlap += overlapcontext( h, sentence )
        if overlap > maxoverlap:
                maxoverlap = overlap
                bestsense = sense
    return bestsense


sentence = "i went to the bank to deposit my money"
word = "bank"

a = lesk(word,sentence)
print("\n\nSynset:",a)
if a is not None:
    print("Meaning:",a.definition())
    num=0
    print("\nExamples:")
    for i in a.examples():
        num=num+1
        print(str(num)+'.'+')',i)

#3

import bs4 as bs
import urllib.request
import re
import nltk

scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome_coronavirus_2')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
sentence_list = nltk.sent_tokenize(article_text)
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}
for word in nltk.word_tokenize(formatted_article_text):
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
    maximum_frequncy = max(word_frequencies.values())
for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
    sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
import heapq
summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)



import nltk
#nltk.download('punkt')
nltk.download('stopwords')

#4

from pprint import pprint
from operator import itemgetter
from nltk.corpus import framenet as fn
from nltk.corpus.reader.framenet import PrettyList
x = fn.frames(r'(?i)crim')
x.sort(key=itemgetter('ID'))
x

from pprint import pprint
from nltk.corpus import framenet as fn
f = fn.frame(202)
f.ID

f.name

f.definition

len(f.lexUnit)

pprint(sorted([x for x in f.FE]))
pprint(f.frameRelations)

PrettyList(sorted(fn.frames_by_lemma(r'(?i)a little'), key=itemgetter('ID')))

from pprint import pprint
PrettyList(sorted(fn.lus(r'(?i)a little'), key=itemgetter('ID')))

from pprint import pprint
from nltk.corpus import framenet as fn
fn.lu(256).name

fn.lu(256).definition

fn.lu(256).frame.name

fn.lu(256).lexemes[0].name

from pprint import pprint
from nltk.corpus import framenet as fn
d = fn.docs('BellRinging')[0]
d.corpname

d.sentence[49]

d.sentence[49].annotationSet[1]

import nltk
nltk.download('framenet_v17')

#5

